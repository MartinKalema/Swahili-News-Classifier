{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('../')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "from swahiliNewsClassifier.constants import *\n",
    "from swahiliNewsClassifier.utilities.helper_functions import read_yaml, create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class DataIngestionConfig:\n",
    "    root_dir: Path\n",
    "    train_source_URL: str\n",
    "    test_source_URL: str\n",
    "    train_data_file: Path\n",
    "    test_data_file: Path\n",
    "    decompressed_dir: Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(self, config_filepath=CONFIG_FILE_PATH, params_filepath=PARAMS_FILE_PATH):\n",
    "        \"\"\"\n",
    "        Initialize ConfigurationManager with configuration and parameter files.\n",
    "\n",
    "        Args:\n",
    "            config_filepath (str): Path to the configuration YAML file.\n",
    "            params_filepath (str): Path to the parameters YAML file.\n",
    "        \"\"\"\n",
    "        self.configurations = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "    def get_data_ingestion_config(self) -> DataIngestionConfig:\n",
    "        \"\"\"\n",
    "        Get the data ingestion configuration.\n",
    "\n",
    "        Returns:\n",
    "            DataIngestionConfig: Configuration object for data ingestion.\n",
    "        \"\"\"\n",
    "        data_ingestion_configurations = self.configurations.data_ingestion\n",
    "\n",
    "        create_directories([data_ingestion_configurations.root_dir])\n",
    "\n",
    "        return DataIngestionConfig(\n",
    "            root_dir=data_ingestion_configurations.root_dir,\n",
    "            train_source_URL=data_ingestion_configurations.train_source_URL,\n",
    "            test_source_URL=data_ingestion_configurations.test_source_URL,\n",
    "            train_data_file=data_ingestion_configurations.train_data_file,\n",
    "            test_data_file=data_ingestion_configurations.test_data_file,\n",
    "            decompressed_dir=data_ingestion_configurations.decompressed_dir\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import gdown\n",
    "from swahiliNewsClassifier import log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataIngestion:\n",
    "    def __init__(self, data_ingestion_configuartions: DataIngestionConfig):\n",
    "        \"\"\"\n",
    "        Initialize DataIngestion object with the provided configuration.\n",
    "\n",
    "        Args:\n",
    "            data_ingestion_configuartions (DataIngestionConfig): Configuration object for data ingestion.\n",
    "        \"\"\"\n",
    "        self.data_ingestion_configuartions = data_ingestion_configuartions\n",
    "\n",
    "    def download_file(self):\n",
    "        \"\"\"Fetch data from a URL.\n",
    "        \n",
    "        Raises:\n",
    "            Exception: If an error occurs during the download process.\n",
    "        \"\"\"\n",
    "        os.makedirs(\"artifacts/data_ingestion/compressed\", exist_ok=True)\n",
    "        os.makedirs(\"artifacts/data_ingestion/decompressed\", exist_ok=True)\n",
    "        dataset_urls = [self.data_ingestion_configuartions.train_source_URL, self.data_ingestion_configuartions.test_source_URL]\n",
    "        zip_download_dir = [self.data_ingestion_configuartions.train_data_file, self.data_ingestion_configuartions.test_data_file]\n",
    "        \n",
    "        for url, dest in zip(dataset_urls, zip_download_dir):\n",
    "            try:\n",
    "                log.info(f\"Downloading data from {url} into file {dest}\")\n",
    "\n",
    "                file_id = url.split(\"/\")[-2]\n",
    "                prefix = \"https://drive.google.com/uc?/export=download&id=\"\n",
    "                gdown.download(prefix + file_id, dest)\n",
    "\n",
    "                log.info(f\"Downloaded data from {url} into file {dest}\")\n",
    "            except Exception as e:\n",
    "                log.error(f\"Error downloading file from {url} to {dest}\")\n",
    "                raise e\n",
    "\n",
    "    def extract_zip_file(self):\n",
    "        \"\"\"Extract a zip file.\n",
    "\n",
    "        This method extracts the contents of a zip file specified in the configuration\n",
    "        to the directory specified in the configuration.\n",
    "\n",
    "        Raises:\n",
    "            Exception: If an error occurs during the extraction process.\n",
    "        \"\"\"\n",
    "        zip_download_dir = [self.data_ingestion_configuartions.train_data_file, self.data_ingestion_configuartions.test_data_file]\n",
    "        decompress_path = self.data_ingestion_configuartions.decompressed_dir\n",
    "        os.makedirs(decompress_path, exist_ok=True)\n",
    "\n",
    "        for zip_file in zip_download_dir:\n",
    "            try:\n",
    "                with zipfile.ZipFile(zip_file, \"r\") as zip_ref:\n",
    "                    zip_ref.extractall(decompress_path)\n",
    "\n",
    "                log.info(f\"Extracted zip file {zip_file} into: {decompress_path}\")\n",
    "            except Exception as e:\n",
    "                log.error(f\"Error extracting zip file: {zip_file}\")\n",
    "                raise e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-06-08 17:23:44,925: INFO: helper_functions:20: yaml file: configuration/configuration.yaml loaded successfully]\n",
      "[2024-06-08 17:23:44,935: INFO: helper_functions:20: yaml file: parameters.yaml loaded successfully]\n",
      "[2024-06-08 17:23:44,942: INFO: helper_functions:35: Created directory at: artifacts]\n",
      "[2024-06-08 17:23:44,945: INFO: helper_functions:35: Created directory at: artifacts/data_ingestion]\n",
      "[2024-06-08 17:23:44,952: INFO: 3987317555:24: Downloading data from https://drive.google.com/file/d/15stuLDZkXNOgBUC1rnx5yXYdVPViUjNB/view?usp=sharing into file artifacts/data_ingestion/compressed/train_data.zip]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?/export=download&id=15stuLDZkXNOgBUC1rnx5yXYdVPViUjNB\n",
      "To: /media/kalema/9954-79C8/Projects/Swahili-News-Classifier/artifacts/data_ingestion/compressed/train_data.zip\n",
      "100%|██████████| 3.78M/3.78M [00:03<00:00, 946kB/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-06-08 17:23:52,445: INFO: 3987317555:30: Downloaded data from https://drive.google.com/file/d/15stuLDZkXNOgBUC1rnx5yXYdVPViUjNB/view?usp=sharing into file artifacts/data_ingestion/compressed/train_data.zip]\n",
      "[2024-06-08 17:23:52,446: INFO: 3987317555:24: Downloading data from https://drive.google.com/file/d/1mjmYzMdnn_UwSEgTQ7i-cJ5WSOokt9Er/view?usp=sharing into file artifacts/data_ingestion/compressed/test_data.zip]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?/export=download&id=1mjmYzMdnn_UwSEgTQ7i-cJ5WSOokt9Er\n",
      "To: /media/kalema/9954-79C8/Projects/Swahili-News-Classifier/artifacts/data_ingestion/compressed/test_data.zip\n",
      "100%|██████████| 992k/992k [00:00<00:00, 1.15MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-06-08 17:23:56,279: INFO: 3987317555:30: Downloaded data from https://drive.google.com/file/d/1mjmYzMdnn_UwSEgTQ7i-cJ5WSOokt9Er/view?usp=sharing into file artifacts/data_ingestion/compressed/test_data.zip]\n",
      "[2024-06-08 17:23:56,455: INFO: 3987317555:53: Extracted zip file artifacts/data_ingestion/compressed/train_data.zip into: artifacts/data_ingestion/decompressed]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-06-08 17:23:56,535: INFO: 3987317555:53: Extracted zip file artifacts/data_ingestion/compressed/test_data.zip into: artifacts/data_ingestion/decompressed]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    data_ingestion_config = config.get_data_ingestion_config()\n",
    "    data_ingestion = DataIngestion(config=data_ingestion_config)\n",
    "    data_ingestion.download_file()\n",
    "    data_ingestion.extract_zip_file()\n",
    "except Exception as e:\n",
    "    raise e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
